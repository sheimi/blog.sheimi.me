---
layout: post
title: 源代码阅读笔记（2） --- nutch (Injector)
category: blog
published: true
meta:
  location: NJU 
tags: [hadoop, study, source_code, architecture, nutch, crawler]
---

上次说道，nutch 的爬虫在运行中的总体的步骤，他们由：

+   Injector
+   Generator
+   Fetcher
+   ParseSegment
+   SolrIndexer

组成。其中 Injector 是一开始的初始化，SolerIndexer 是迭代结束的时候再开始的。而中间的三步是放在一个 for 循环里的。

下面就一个一个来分析。


Injector
--------

Injector 是一个 MapReduce 的程序，它的作用是把用户提交的初始的 url 列表注入到 CrawlDb 中. 并且 Nutch 是要定期更新爬取的东西，所以，Injector 中还对新的 url 和老的 url 做了一些合并。

CrawlDb 是 Nutch 中储存 url 的数据库的数据结构，包含了 url 的一些 metadata，包括时间，爬取状态等信息。

下面是 Injector Job 的配置,

{% highlight java %}
if (LOG.isInfoEnabled()) {
  LOG.info("Injector: Converting injected urls to crawl db entries.");
}
JobConf sortJob = new NutchJob(getConf());
sortJob.setJobName("inject " + urlDir);
FileInputFormat.addInputPath(sortJob, urlDir);
sortJob.setMapperClass(InjectMapper.class);

FileOutputFormat.setOutputPath(sortJob, tempDir);
sortJob.setOutputFormat(SequenceFileOutputFormat.class);
sortJob.setOutputKeyClass(Text.class);
sortJob.setOutputValueClass(CrawlDatum.class);
sortJob.setLong("injector.current.time", System.currentTimeMillis());
JobClient.runJob(sortJob);

// merge with existing crawl db
if (LOG.isInfoEnabled()) {
  LOG.info("Injector: Merging injected urls into crawl db.");
}
JobConf mergeJob = CrawlDb.createJob(getConf(), crawlDb);
FileInputFormat.addInputPath(mergeJob, tempDir);
mergeJob.setReducerClass(InjectReducer.class);
JobClient.runJob(mergeJob);
CrawlDb.install(mergeJob, crawlDb);

// clean up
FileSystem fs = FileSystem.get(getConf());
fs.delete(tempDir, true);

long end = System.currentTimeMillis();
LOG.info("Injector: finished at " + sdf.format(end) + ", elapsed: " + TimingUtil.elapsedTime(start, end));
{% endhighlight %}

从上面的代码的中可以看出，Injector运行了两个 MapReduce Job：

+   第一个使用了 Injector Mapper 类，把用户一开始入口 url 地址转化为 CrawlDatum(这个是 CrawlDB 的里面的一项)，输出的格式是 SequenceFile
+   第二个使用了 Injector Reducer 类，把第一次的输出当作输入，作用是把老的数据库和新的数据库进行合并。

接下来一个一个地分析

### 第一个MapReduce

代码比较长，就不贴在这里了，这里就列一下代码所做的一些工作：

+   对 url 进行一些处理
+   如果 url 有 metadata 就把 metadata 提取出来
+   根据既定的一些规则对 url 进行过滤
+   如果通过过滤，则把它收集记起来（存储到CrawlDB中）
+   存储的方式是 Key 为 url 本身，Value 为 url 的CrawlDatum

### 第二个MapReduce

第二个MapReduce中少了一部分配置的代码，从上面的代码看，还以为没有设置 Mapper 类。其实，它是通过一个 CrawlDb 中的一个 static 方法建立的，这个方法首先首先初始化了一下配置。下面是代码：

{% highlight java %}
public static JobConf createJob(Configuration config, Path crawlDb) 
      throws IOException {
  Path newCrawlDb = new Path(crawlDb, 
                             Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));

  JobConf job = new NutchJob(config);
  job.setJobName("crawldb " + crawlDb);

  Path current = new Path(crawlDb, CURRENT_NAME);
  if (FileSystem.get(job).exists(current)) {
    FileInputFormat.addInputPath(job, current);
  }
  job.setInputFormat(SequenceFileInputFormat.class);

  job.setMapperClass(CrawlDbFilter.class);
  job.setReducerClass(CrawlDbReducer.class);

  FileOutputFormat.setOutputPath(job, newCrawlDb);
  job.setOutputFormat(MapFileOutputFormat.class);
  job.setOutputKeyClass(Text.class);
  job.setOutputValueClass(CrawlDatum.class);

  // https://issues.apache.org/jira/browse/NUTCH-1110
  job.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);

  return job;
}
{% endhighlight %}

+   从上面的代码可以分析，推测出，第二个 Job 的输入的路径是以前和第一个 Job 的输出。
+   Mapper 类是 CrawlDbFilter 。它对输入的 url 进行了筛选。
+   Reducer 类是原来的InjectReducer，作用是把新的 url 合并到老的 url 中，并把新的标记为 UNFECTCHED ，老的不会被覆盖。

再回到主程序，当两个 Job 运行完后，会调用 CrawlDb.install 方法：

{% highlight java %}
public static void install(JobConf job, Path crawlDb) throws IOException {
  Path newCrawlDb = FileOutputFormat.getOutputPath(job);
  FileSystem fs = new JobClient(job).getFs();
  Path old = new Path(crawlDb, "old");
  Path current = new Path(crawlDb, CURRENT_NAME);
  if (fs.exists(current)) {
    if (fs.exists(old)) fs.delete(old, true);
    fs.rename(current, old);
  }
  fs.mkdirs(crawlDb);
  fs.rename(newCrawlDb, current);
  if (fs.exists(old)) fs.delete(old, true);
  Path lock = new Path(crawlDb, LOCK_NAME);
  LockUtil.removeLockFile(fs, lock);
}
{% endhighlight %}

着一个方法把 crawDb 的一些文件夹重命名了一下，并且揭开了文件琐。
最后主程序会吧第一个 MapReduce 产生的临时文件删除。


To Be Continued ~~~

